Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 501])
Labels Shape: torch.Size([64, 30])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([26, 26, 25, 20, 27, 21, 25, 19, 20, 22, 16, 16, 20, 23, 13, 22, 14, 21,
        15, 15, 17, 19, 22, 25, 29, 18, 21, 17, 16, 14, 24, 18, 15, 16, 17, 21,
        20, 22, 18, 17, 22, 15, 21, 15, 20, 30, 27, 14, 19, 17, 26, 29, 20, 29,
        27, 16, 17, 18, 26, 23, 13, 18, 21, 28])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Batch Loss: 83.7828
Target: [283, 288, 685, 468, 793, 107, 84, 667, 21, 66, 180, 779, 39, 361, 19, 3, 151, 38, 790, 78, 102, 461, 760, 520, 13, 23, 0, 0, 0, 0] 
Predicted: [578, 393, 739, 807, 322, 62, 789, 789, 318, 523, 523, 789, 789, 523, 122, 789, 775, 376, 789, 318, 376, 789, 376, 789, 62, 376, 789, 318, 376, 807, 201, 471, 523, 376, 201, 523, 471, 376, 62, 376, 789, 376, 807, 376, 318, 471, 615, 322, 789, 376, 376, 789, 789, 523, 789, 523, 789, 523, 318, 523, 523, 726, 471, 782, 384, 62, 201, 376, 523, 376, 526, 526, 526, 376, 376, 789, 789, 201, 523, 376, 376, 376, 741, 376, 615, 284, 523, 789, 471, 578, 376, 86, 523, 322, 376, 789, 376, 376, 807, 789, 807, 517, 376, 376, 398, 376, 702, 376, 807, 807, 807, 807, 789, 376, 615, 615, 376, 376, 523, 523, 523, 523, 523, 789, 318, 284, 239, 376, 789, 523, 376, 789, 789, 807, 807, 650, 789, 789, 615, 789, 807, 807, 260, 807, 523, 201, 807, 807, 615, 789, 376, 615, 789, 201, 439, 376, 201, 376, 376, 789, 376, 789, 86, 376, 376, 62, 376, 318, 376, 789, 523, 807, 471, 376, 523, 615, 523, 615, 523, 376, 523, 789, 523, 86, 413, 376, 376, 376, 376, 206, 376, 523, 201, 376, 859, 789, 789, 789, 523, 789, 549, 618, 376, 471, 789, 618, 201, 376, 523, 376, 376, 523, 523, 376, 376, 376, 463, 853, 730, 206, 393, 206, 206, 523, 194, 523, 523, 730, 523, 376, 523, 523, 523, 173, 322, 523, 318, 523, 318, 523, 284, 376, 376, 376, 269, 206, 523, 900, 523, 523, 526]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 2 / 20]
Batch Loss: 64.0914
Target: [15, 115, 317, 14, 235, 712, 32, 525, 178, 9, 136, 785, 8, 96, 7, 696, 32, 105, 74, 382, 190, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 3 / 20]
Batch Loss: 43.5682
Target: [7, 727, 113, 779, 320, 69, 637, 368, 20, 636, 336, 797, 8, 12, 265, 47, 777, 117, 406, 781, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 4 / 20]
Batch Loss: 21.4558
Target: [48, 784, 144, 74, 775, 53, 112, 5, 607, 474, 161, 70, 8, 39, 759, 87, 496, 64, 27, 779, 35, 210, 86, 19, 170, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 5 / 20]
Batch Loss: 8.0806
Target: [61, 35, 119, 340, 674, 17, 23, 244, 7, 161, 14, 779, 39, 35, 777, 69, 789, 234, 215, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 6 / 20]
Batch Loss: 7.6581
Target: [411, 284, 157, 540, 235, 661, 184, 7, 160, 775, 259, 231, 776, 75, 779, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 601])
Labels Shape: torch.Size([64, 36])
Features Length: tensor([601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601])
Shape: torch.Size([64])
Labels Length: tensor([27, 14, 23, 28, 18, 22, 18, 23, 22, 27, 31, 24, 31, 27, 32, 27, 25, 28,
        36, 30, 25, 21, 26, 22, 26, 19, 24, 21, 30, 24, 23, 21, 25, 33, 30, 16,
        15, 29, 17, 25, 28, 15, 33, 26, 26, 35, 29, 25, 16, 27, 28, 27, 23, 24,
        20, 28, 26, 25, 21, 26, 21, 15, 34, 22])
Shape: torch.Size([64])
[Epoch 1] - [Batch 7 / 20]
Batch Loss: 8.6986
Target: [46, 112, 623, 138, 5, 35, 222, 32, 34, 47, 786, 797, 774, 38, 454, 100, 7, 240, 787, 12, 275, 72, 793, 778, 797, 38, 454, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 8 / 20]
Batch Loss: 8.8143
Target: [7, 25, 200, 309, 112, 118, 126, 14, 73, 7, 17, 135, 793, 778, 171, 38, 789, 31, 62, 79, 82, 727, 113, 779, 35, 120, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 9 / 20]
Batch Loss: 8.2285
Target: [112, 30, 296, 79, 394, 84, 7, 65, 24, 787, 25, 78, 11, 783, 17, 94, 791, 107, 25, 78, 11, 783, 39, 143, 174, 63, 55, 790, 790, 14, 51, 94, 783, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 10 / 20]
Batch Loss: 7.1366
Target: [5, 758, 181, 156, 467, 55, 682, 5, 108, 775, 189, 41, 120, 587, 40, 792, 678, 123, 783, 13, 472, 159, 344, 565, 66, 107, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 501])
Labels Shape: torch.Size([64, 33])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([22, 20, 19, 16, 21, 24, 30, 19, 25, 15, 28, 27, 20, 20, 25, 20, 27, 16,
        21, 17, 25, 23, 19, 20, 30, 20, 16, 22, 21, 17, 18, 27, 29, 25, 15, 20,
        16, 21, 16, 19, 21, 20, 33, 28, 28, 24, 19, 21, 16, 23, 13, 12, 10, 20,
        25, 14, 20, 30, 15, 22, 23, 17, 26, 21])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Batch Loss: 80.2771
Target: [7, 665, 127, 343, 23, 7, 25, 19, 62, 776, 432, 8, 794, 81, 627, 73, 199, 15, 23, 301, 464, 178, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [70, 903, 533, 308, 766, 766, 561, 561, 377, 533, 766, 766, 714, 584, 305, 822, 949, 949, 949, 949, 949, 949, 949, 949, 949, 949, 949, 949, 949, 756, 378, 756, 756, 949, 221, 305, 949, 484, 949, 601, 887, 305, 305, 305, 484, 15, 424, 686, 686, 686, 15, 887, 378, 983, 15, 601, 561, 561, 411, 305, 305, 15, 49, 561, 411, 484, 601, 949, 676, 949, 949, 937, 411, 949, 533, 949, 937, 949, 949, 949, 305, 949, 949, 949, 756, 949, 822, 949, 455, 484, 949, 455, 949, 223, 949, 949, 949, 601, 949, 949, 756, 686, 822, 949, 676, 221, 939, 949, 394, 756, 949, 15, 15, 937, 15, 411, 15, 822, 822, 699, 305, 699, 949, 949, 949, 949, 949, 822, 949, 949, 756, 305, 949, 501, 15, 378, 223, 949, 949, 193, 676, 949, 561, 949, 561, 561, 949, 70, 822, 699, 15, 411, 15, 15, 686, 788, 601, 822, 822, 887, 822, 822, 822, 822, 305, 424, 424, 484, 756, 937, 676, 15, 15, 949, 601, 822, 424, 949, 949, 949, 223, 949, 15, 378, 949, 949, 949, 983, 15, 15, 15, 411, 305, 305, 561, 561, 193, 746, 15, 447, 686, 686, 686, 686, 561, 193, 949, 816, 561, 193, 193, 746, 223, 223, 484, 887, 949, 949, 949, 949, 561, 949, 949, 949, 983, 533, 949, 627, 949, 533, 568, 15, 949, 70, 15, 15, 15, 15, 533, 533, 15, 15, 533, 15, 533, 15, 756, 15, 887, 15, 887]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 2 / 20]
Batch Loss: 31.3099
Target: [61, 35, 119, 340, 674, 17, 23, 244, 7, 161, 14, 779, 39, 35, 777, 69, 789, 234, 215, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 3 / 20]
Batch Loss: 12.2105
Target: [43, 530, 779, 3, 147, 83, 63, 53, 7, 411, 6, 785, 206, 28, 32, 109, 272, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 4 / 20]
Batch Loss: 6.6730
Target: [109, 44, 128, 59, 89, 302, 304, 30, 3, 10, 28, 127, 46, 780, 89, 3, 10, 28, 21, 116, 80, 221, 215, 780, 71, 64, 318, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 5 / 20]
Batch Loss: 82.8339
Target: [610, 12, 243, 793, 10, 7, 17, 275, 233, 296, 641, 32, 26, 792, 93, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [770, 564, 564, 770, 770, 819, 564, 564, 564, 564, 564, 770, 564, 564, 564, 910, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 811, 564, 819, 770, 770, 564, 564, 811, 564, 811, 564, 564, 564, 564, 564, 564, 564, 819, 811, 564, 564, 770, 564, 564, 770, 564, 564, 564, 564, 564, 564, 819, 564, 564, 819, 811, 564, 770, 564, 434, 564, 928, 564, 564, 811, 564, 811, 811, 564, 564, 545, 564, 770, 584, 819, 770, 564, 564, 770, 819, 770, 819, 811, 564, 770, 564, 564, 770, 564, 819, 811, 770, 819, 564, 564, 564, 434, 564, 564, 564, 564, 564, 564, 564, 910, 910, 770, 811, 564, 811, 564, 434, 564, 564, 995, 564, 819, 811, 564, 564, 564, 564, 564, 770, 811, 564, 564, 564, 811, 564, 564, 564, 770, 887, 564, 811, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 811, 564, 564, 564, 910, 564, 564, 564, 564, 811, 564, 564, 564, 564, 811, 564, 770, 564, 811, 819, 564, 819, 770, 564, 564, 564, 995, 819, 811, 910, 564, 564, 564, 564, 564, 564, 564, 811, 564, 564, 564, 811, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 811, 770, 564, 819, 434, 811, 770, 770, 564, 564, 434, 564, 564, 819, 564, 564, 770, 564, 434, 564, 564, 564, 770, 564, 819, 564, 564, 564, 770, 811, 564, 564, 928, 819, 564, 564, 564, 564, 564, 819, 564, 811, 564]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 6 / 20]
Batch Loss: 69.8240
Target: [7, 772, 351, 31, 141, 32, 751, 14, 10, 190, 44, 498, 73, 7, 9, 140, 339, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [284, 184, 184, 643, 184, 759, 658, 798, 284, 125, 184, 798, 798, 184, 184, 184, 184, 759, 184, 643, 798, 798, 184, 284, 296, 184, 184, 184, 184, 798, 798, 184, 798, 284, 798, 184, 798, 184, 759, 184, 281, 643, 798, 798, 184, 184, 643, 798, 184, 184, 184, 798, 675, 284, 229, 798, 184, 184, 184, 184, 798, 281, 296, 643, 643, 284, 184, 284, 798, 184, 798, 184, 284, 798, 184, 45, 184, 184, 184, 184, 184, 184, 643, 281, 184, 284, 184, 281, 284, 184, 184, 798, 798, 184, 184, 281, 184, 798, 184, 284, 643, 229, 184, 184, 184, 643, 184, 658, 643, 643, 184, 507, 281, 281, 798, 184, 658, 184, 507, 798, 281, 184, 184, 184, 184, 798, 284, 759, 184, 798, 284, 281, 184, 184, 643, 798, 281, 184, 142, 184, 184, 184, 184, 798, 798, 184, 184, 507, 798, 798, 184, 643, 759, 798, 184, 281, 759, 759, 184, 281, 770, 184, 281, 281, 798, 184, 759, 798, 284, 184, 798, 798, 284, 798, 643, 184, 759, 184, 798, 184, 184, 798, 281, 184, 798, 798, 184, 675, 643, 184, 184, 467, 184, 184, 284, 798, 643, 184, 184, 284, 399, 284, 184, 643, 281, 798, 184, 184, 798, 798, 643, 798, 658, 759, 658, 507, 284, 281, 184, 798, 798, 643, 798, 295, 184, 281, 184, 284, 184, 798, 643, 643, 281, 798, 281, 184, 295, 184, 184, 184, 798, 184, 759, 184, 643, 759, 798, 798, 229, 798, 798]
----------------------------------------------------------------------------------------------------
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 601])
Labels Shape: torch.Size([64, 40])
Features Length: tensor([601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601])
Shape: torch.Size([64])
Labels Length: tensor([22, 20, 24, 24, 29, 28, 32, 19, 30, 24, 28, 27, 21, 22, 27, 28, 25, 22,
        15, 24, 29, 25, 19, 23, 27, 23, 23, 25, 18, 26, 16, 22, 14, 33, 27, 28,
        19, 33, 12, 13, 20, 14, 40, 27, 35, 27, 20, 20, 33, 22, 20, 32, 33, 27,
        23, 25, 21, 22, 27, 20, 26, 26, 26, 23])
Shape: torch.Size([64])
[Epoch 1] - [Batch 7 / 20]
Batch Loss: 71.5572
Target: [9, 699, 3, 389, 779, 510, 44, 432, 79, 720, 407, 779, 244, 32, 7, 295, 785, 349, 248, 380, 774, 795, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [284, 284, 284, 284, 284, 284, 281, 229, 229, 229, 229, 284, 281, 284, 281, 284, 281, 284, 229, 229, 229, 284, 284, 284, 284, 229, 281, 284, 284, 284, 284, 284, 229, 72, 284, 284, 284, 281, 229, 45, 284, 284, 229, 284, 229, 284, 284, 284, 229, 229, 284, 284, 284, 284, 284, 281, 229, 229, 284, 284, 284, 284, 284, 284, 284, 284, 284, 284, 229, 229, 284, 229, 284, 284, 284, 284, 284, 284, 284, 284, 229, 281, 229, 281, 284, 321, 281, 281, 284, 290, 284, 284, 281, 284, 284, 229, 281, 284, 284, 284, 284, 284, 284, 281, 229, 284, 284, 45, 284, 284, 284, 284, 281, 284, 284, 284, 229, 284, 284, 284, 284, 284, 229, 229, 284, 284, 284, 229, 229, 284, 290, 281, 281, 229, 284, 281, 229, 284, 284, 284, 284, 281, 229, 229, 229, 284, 284, 281, 284, 229, 284, 284, 284, 229, 284, 284, 281, 284, 284, 284, 45, 229, 281, 284, 281, 284, 281, 284, 281, 284, 284, 281, 284, 284, 284, 284, 284, 284, 45, 281, 229, 284, 281, 229, 284, 284, 284, 229, 284, 284, 284, 284, 229, 284, 284, 229, 284, 284, 284, 281, 281, 284, 284, 284, 284, 284, 284, 284, 284, 284, 284, 229, 229, 284, 229, 284, 284, 284, 284, 281, 284, 284, 284, 284, 229, 281, 281, 229, 284, 229, 281, 290, 229, 284, 284, 290, 284, 229, 284, 45, 284, 281, 284, 229, 284, 284, 284, 284, 229, 284, 284, 281, 284, 229, 290, 284, 284, 284, 284, 284, 284, 229, 229, 229, 284, 284, 229, 284, 284, 284, 281, 45, 284, 284, 229, 284, 45, 284, 281, 284, 284, 284, 284, 284, 284, 284, 284, 284, 290, 284, 284, 45, 229, 284, 229, 229, 281, 229, 284, 229, 284]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 8 / 20]
Batch Loss: 59.9031
Target: [509, 349, 37, 38, 114, 133, 83, 718, 774, 26, 126, 6, 131, 130, 162, 14, 263, 10, 172, 638, 208, 41, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [7, 66, 7, 153, 5, 7, 7, 7, 5, 5, 221, 41, 117, 7, 30, 5, 7, 153, 7, 41, 221, 66, 5, 7, 7, 7, 5, 7, 66, 8, 41, 66, 8, 153, 7, 7, 7, 7, 66, 7, 7, 221, 7, 66, 30, 7, 41, 7, 41, 7, 7, 7, 5, 7, 153, 7, 7, 66, 7, 7, 221, 189, 221, 66, 153, 221, 7, 7, 66, 41, 5, 221, 7, 7, 7, 7, 66, 8, 29, 8, 7, 7, 8, 5, 221, 66, 5, 221, 41, 7, 41, 5, 153, 5, 7, 7, 41, 189, 7, 221, 29, 221, 66, 5, 66, 41, 29, 7, 8, 66, 66, 41, 7, 38, 7, 5, 8, 7, 189, 66, 5, 7, 153, 7, 5, 7, 5, 7, 66, 66, 7, 7, 7, 66, 7, 7, 7, 153, 7, 38, 66, 7, 66, 7, 7, 189, 5, 720, 5, 5, 29, 8, 117, 66, 7, 7, 7, 5, 5, 66, 7, 5, 7, 8, 7, 5, 41, 117, 153, 7, 41, 7, 66, 8, 221, 38, 117, 41, 7, 7, 221, 7, 153, 66, 7, 38, 5, 7, 7, 221, 7, 153, 7, 153, 8, 7, 5, 66, 7, 7, 8, 720, 66, 7, 7, 7, 7, 221, 7, 7, 66, 66, 66, 7, 7, 41, 38, 7, 66, 66, 153, 7, 7, 38, 66, 301, 301, 117, 66, 7, 38, 8, 5, 5, 5, 38, 8, 66, 66, 66, 221, 7, 66, 66, 8, 41, 153, 5, 7, 66, 720, 8, 41, 66, 5, 7, 7, 7, 66, 7, 7, 221, 38, 221, 117, 66, 153, 221, 41, 7, 153, 8, 29, 7, 66, 8, 66, 7, 7, 8, 29, 5, 7, 7, 66, 8, 7, 7, 7, 720, 66, 5, 7, 7, 41, 7, 153, 7, 5, 5, 7]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 9 / 20]
Batch Loss: 26.9795
Target: [7, 365, 331, 285, 688, 8, 197, 63, 779, 68, 669, 127, 38, 64, 463, 140, 723, 668, 606, 115, 181, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 14, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 10 / 20]
Batch Loss: 25.9548
Target: [242, 9, 671, 133, 122, 462, 114, 792, 55, 44, 161, 479, 770, 24, 28, 13, 14, 39, 12, 66, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 11 / 20]
Batch Loss: 14.5549
Target: [109, 44, 7, 363, 156, 197, 63, 779, 449, 38, 689, 528, 97, 21, 94, 783, 25, 71, 49, 172, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 7, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 12 / 20]
Batch Loss: 16.3389
Target: [639, 446, 14, 104, 7, 3, 297, 40, 627, 779, 509, 117, 14, 12, 265, 792, 780, 32, 159, 792, 37, 38, 114, 135, 97, 12, 339, 8, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 651])
Labels Shape: torch.Size([64, 41])
Features Length: tensor([651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651])
Shape: torch.Size([64])
Labels Length: tensor([30, 29, 28, 23, 29, 16, 24, 27, 35, 23, 29, 25, 22, 23, 27, 29, 30, 26,
        25, 32, 18, 15, 24, 28, 27, 24, 25, 30, 23, 24, 19, 29, 25, 31, 36, 31,
        41, 25, 26, 25, 21, 28, 29, 20, 21, 19, 23, 26, 23, 29, 27, 27, 26, 22,
        23, 26, 26, 23, 18, 21, 30, 29, 28, 29])
Shape: torch.Size([64])
[Epoch 1] - [Batch 13 / 20]
Batch Loss: 15.7856
Target: [329, 780, 683, 154, 792, 44, 5, 460, 782, 111, 695, 14, 30, 44, 390, 47, 38, 46, 783, 789, 277, 226, 60, 651, 638, 412, 238, 368, 47, 281, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 14 / 20]
Batch Loss: 11.3125
Target: [91, 113, 787, 47, 779, 261, 254, 73, 770, 136, 781, 20, 40, 392, 429, 761, 35, 505, 55, 321, 49, 41, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 15 / 20]
Batch Loss: 26.1514
Target: [5, 368, 791, 35, 183, 159, 200, 779, 145, 17, 9, 722, 3, 149, 19, 153, 109, 559, 324, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 793, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 774, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 112, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 16 / 20]
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 501])
Labels Shape: torch.Size([64, 30])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([18, 22, 20, 22, 28, 19, 18, 12, 17, 24, 25, 16, 25, 25, 22, 29, 20, 25,
        22, 14, 23, 18, 17, 27, 24, 19, 18, 24, 22, 25, 24, 25, 22, 15, 22, 21,
        30, 13, 20, 13, 24, 23, 27, 21, 26, 19, 19, 22, 27, 16, 23, 14, 14, 23,
        20, 20, 16, 23, 16, 20, 22, 14, 14, 25])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Batch Loss: 81.5770
Target: [38, 25, 699, 97, 25, 699, 281, 46, 318, 312, 701, 21, 37, 107, 39, 491, 117, 472, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [709, 364, 364, 364, 364, 364, 40, 364, 154, 459, 154, 946, 709, 154, 946, 364, 512, 248, 3, 404, 814, 252, 364, 252, 364, 404, 40, 364, 404, 364, 364, 3, 946, 482, 831, 445, 3, 364, 3, 364, 709, 404, 364, 154, 227, 364, 359, 512, 364, 913, 364, 364, 364, 852, 481, 168, 158, 824, 845, 79, 79, 252, 79, 903, 154, 903, 154, 482, 739, 983, 287, 158, 364, 3, 154, 225, 481, 946, 903, 638, 252, 481, 404, 481, 946, 481, 454, 946, 154, 3, 404, 92, 92, 364, 820, 364, 946, 870, 92, 861, 861, 318, 364, 364, 946, 861, 820, 946, 930, 946, 167, 481, 364, 25, 364, 252, 364, 252, 364, 364, 3, 364, 364, 852, 861, 861, 861, 364, 861, 831, 946, 997, 725, 459, 364, 946, 49, 946, 3, 946, 946, 3, 3, 946, 92, 481, 3, 92, 978, 92, 364, 481, 364, 364, 364, 946, 946, 983, 287, 482, 946, 828, 364, 709, 287, 54, 190, 946, 481, 38, 481, 190, 946, 252, 154, 92, 946, 154, 946, 946, 190, 459, 343, 825, 3, 946, 3, 364, 946, 364, 3, 958, 946, 789, 261, 364, 3, 3, 481, 3, 3, 364, 481, 364, 158, 364, 168, 481, 863, 92, 3, 946, 814, 364, 946, 946, 512, 154, 824, 364, 154, 364, 863, 154, 863, 978, 978, 364, 25, 423, 978, 903, 79, 824, 364, 52, 586, 481, 576, 364, 283, 481, 709, 576, 364, 709, 709, 946, 364, 824, 709]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 2 / 20]
Batch Loss: 42.4829
Target: [7, 543, 470, 5, 137, 777, 133, 145, 753, 422, 14, 164, 185, 223, 32, 7, 470, 31, 24, 88, 787, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 3 / 20]
Batch Loss: 10.4869
Target: [46, 476, 15, 10, 96, 7, 91, 117, 779, 248, 554, 72, 294, 78, 37, 30, 26, 133, 785, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 4 / 20]
Batch Loss: 23.5653
Target: [7, 118, 545, 787, 208, 782, 206, 53, 5, 385, 54, 85, 381, 450, 284, 543, 356, 38, 156, 32, 35, 349, 35, 210, 86, 19, 170, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 5 / 20]
Batch Loss: 9.1469
Target: [155, 432, 79, 107, 25, 776, 174, 776, 201, 21, 195, 24, 319, 21, 256, 39, 35, 371, 23, 159, 195, 47, 37, 38, 31, 327, 776, 777, 777, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 6 / 20]
Batch Loss: 6.4118
Target: [5, 406, 789, 49, 107, 420, 784, 37, 91, 94, 8, 44, 5, 632, 85, 798, 110, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 601])
Labels Shape: torch.Size([64, 40])
Features Length: tensor([601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601])
Shape: torch.Size([64])
Labels Length: tensor([28, 20, 28, 24, 24, 21, 21, 22, 14, 25, 32, 22, 22, 26, 29, 19, 29, 22,
        26, 25, 18, 26, 26, 22, 29, 22, 24, 24, 18, 30, 18, 20, 22, 25, 26, 23,
        33, 24, 20, 21, 22, 33, 27, 24, 26, 13, 40, 28, 21, 16, 25, 34, 24, 29,
        16, 31, 30, 25, 16, 24, 22, 20, 25, 22])
Shape: torch.Size([64])
[Epoch 1] - [Batch 7 / 20]
Batch Loss: 24.2107
Target: [288, 58, 123, 50, 776, 557, 358, 39, 727, 113, 779, 396, 147, 49, 14, 12, 675, 93, 69, 251, 774, 28, 73, 7, 607, 118, 126, 779, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 8 / 20]
Batch Loss: 10.0173
Target: [46, 53, 451, 30, 12, 50, 793, 66, 776, 43, 777, 110, 27, 776, 485, 245, 143, 718, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 9 / 20]
Batch Loss: 7.7814
Target: [7, 35, 347, 250, 94, 302, 247, 73, 7, 91, 4, 415, 77, 787, 44, 7, 521, 749, 34, 85, 621, 231, 473, 30, 7, 630, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 10 / 20]
Batch Loss: 8.6300
Target: [169, 505, 10, 15, 171, 44, 498, 96, 7, 756, 15, 23, 301, 464, 178, 32, 773, 206, 779, 791, 375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 11 / 20]
Batch Loss: 7.4907
Target: [15, 281, 38, 7, 756, 39, 612, 145, 152, 775, 589, 517, 109, 9, 146, 316, 777, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 12 / 20]
Batch Loss: 9.3075
Target: [288, 32, 348, 734, 8, 230, 145, 551, 70, 785, 213, 35, 183, 73, 7, 91, 70, 499, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
----------------------------------------------------------------------------------------------------
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 651])
Labels Shape: torch.Size([64, 41])
Features Length: tensor([651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651])
Shape: torch.Size([64])
Labels Length: tensor([24, 27, 26, 28, 30, 23, 18, 28, 24, 23, 25, 26, 22, 20, 41, 19, 33, 21,
        23, 26, 22, 29, 32, 25, 24, 31, 26, 25, 20, 37, 27, 22, 24, 28, 25, 26,
        33, 22, 20, 23, 19, 24, 18, 23, 25, 32, 30, 23, 27, 20, 29, 26, 36, 22,
        27, 25, 18, 26, 27, 24, 22, 30, 22, 25])
Shape: torch.Size([64])
[Epoch 1] - [Batch 13 / 20]
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 501])
Labels Shape: torch.Size([64, 34])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([20, 25, 25, 21, 15, 17, 22, 23, 21, 19, 16, 16, 16, 16, 34, 15, 17, 18,
        22, 19, 18, 15, 24, 12, 18, 26, 25, 22, 17, 12, 19, 14, 19, 25, 21, 17,
        12, 22, 18, 16, 25, 22, 20, 12, 23, 19, 20, 18, 22, 17, 20, 15, 26, 33,
        16, 23, 21, 22, 14, 20, 15, 16, 23, 27])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
  (test_gru): GRU(512, 512, batch_first=True, dropout=0.1, bidirectional=True)
)
Model Parameters: 17489480
Features Shape: torch.Size([64, 80, 501])
Labels Shape: torch.Size([64, 34])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([26, 22, 19, 30, 17, 18, 22, 12, 25, 22, 27, 24, 28, 16, 21, 17, 12, 26,
        21, 23, 19, 21, 17, 25, 23, 18, 28, 16, 21, 21, 18, 16, 22, 17, 18, 21,
        23, 27, 24, 17, 34, 22, 14, 20, 18, 26, 28, 16, 21, 27, 28, 21, 18, 24,
        26, 22, 27, 18, 26, 25, 23, 15, 22, 25])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
  (test_gru): GRU(512, 512, batch_first=True, dropout=0.1, bidirectional=True)
)
Model Parameters: 17489480
Features Shape: torch.Size([64, 80, 501])
Labels Shape: torch.Size([64, 30])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([19, 20, 22, 26, 30, 20, 22, 18, 17, 22, 18, 22, 20, 17, 20, 25, 16, 19,
        17, 21, 21, 21, 25, 30, 23, 20, 17, 20, 18, 14, 16, 26, 15, 21, 23, 24,
        23, 23, 26, 20, 15, 22, 22, 23, 21, 20, 16, 20, 21, 19, 22, 15, 22, 14,
        27, 16, 14, 17, 16, 16, 26, 20, 22, 17])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
  (test_gru): GRU(512, 512, batch_first=True, dropout=0.1, bidirectional=True)
)
Model Parameters: 17489480
Features Shape: torch.Size([64, 80, 501])
Labels Shape: torch.Size([64, 33])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([22, 17, 26, 25, 21, 21, 20, 21, 20, 21, 20, 28, 16, 22, 16, 22, 26, 17,
        29, 20, 12, 30, 21, 24, 19, 19, 15, 15, 27, 17, 26, 15, 33, 25, 22, 17,
        30, 20, 17, 17, 16, 16, 21, 24, 27, 21, 23, 24, 27, 22, 16, 22, 14, 26,
        20, 22, 31, 22, 19, 16, 26, 14, 23, 27])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=80, out_features=40, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=40, out_features=1000, bias=True)
  )
  (test_gru): GRU(80, 40, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)
)
Model Parameters: 13402608
Features Shape: torch.Size([64, 80, 501])
Labels Shape: torch.Size([64, 34])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([13, 23, 20, 22, 24, 21, 19, 19, 22, 20, 20, 16, 25, 19, 30, 19, 20, 13,
        22, 20, 22, 20, 21, 22, 18, 22, 15, 21, 14, 27, 15, 25, 24, 22, 25, 16,
        26, 16, 17, 17, 20, 22, 23, 20, 34, 15, 16, 24, 18, 20, 25, 21, 18, 17,
        30, 22, 20, 28, 21, 28, 22, 17, 22, 15])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=80, out_features=40, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=40, out_features=1000, bias=True)
  )
  (test_gru): GRU(80, 40, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)
)
Model Parameters: 13402608
Features Shape: torch.Size([64, 80, 501])
Labels Shape: torch.Size([64, 30])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([25, 17, 16, 22, 22, 21, 22, 25, 20, 27, 19, 21, 12, 23, 22, 15, 15, 15,
        20, 19, 17, 17, 16, 13, 22, 18, 15, 20, 18, 26, 20, 25, 19, 19, 23, 24,
        20, 16, 21, 24, 20, 21, 18, 20, 20, 17, 24, 24, 22, 17, 10, 14, 19, 30,
        15, 15, 21, 19, 21, 23, 17, 16, 17, 26])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Batch Loss: 85.5279
Target: [7, 17, 36, 55, 794, 171, 457, 17, 19, 774, 788, 18, 153, 773, 76, 790, 78, 81, 32, 659, 333, 49, 46, 166, 230, 0, 0, 0, 0, 0] 
Predicted: [491, 491, 767, 491, 491, 767, 491, 356, 93, 356, 767, 767, 767, 491, 491, 491, 491, 767, 767, 491, 767, 767, 491, 491, 491, 767, 767, 767, 356, 767, 767, 491, 767, 767, 491, 491, 356, 767, 491, 356, 767, 491, 767, 491, 491, 491, 491, 24, 491, 767, 491, 767, 767, 491, 767, 356, 767, 491, 491, 767, 767, 491, 356, 491, 767, 767, 767, 767, 767, 356, 767, 767, 837, 767, 356, 767, 356, 356, 356, 767, 767, 767, 93, 767, 491, 767, 491, 767, 767, 767, 491, 767, 113, 767, 767, 767, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 956, 491, 491, 258, 767, 258, 491, 767, 491, 491, 767, 767, 491, 767, 491, 615, 24, 767, 767, 767, 767, 491, 837, 356, 491, 767, 24, 767, 767, 491, 767, 810, 767, 24, 24, 491, 491, 491, 491, 24, 491, 491, 24, 491, 356, 767, 767, 767, 356, 356, 767, 491, 24, 491, 491, 491, 491, 767, 767, 767, 767, 767, 767, 615, 767, 767, 767, 767, 767, 767, 491, 767, 491, 767, 93, 767, 491, 767, 767, 767, 93, 356, 767, 491, 767, 767, 356, 767, 767, 767, 767, 356, 767, 491, 491, 491, 24, 24, 491, 767, 356, 356, 491, 767, 24, 356, 615, 767, 491, 24, 767, 767, 491, 491, 767, 767, 767, 356, 767, 767, 767, 491, 491, 767, 491, 767, 356, 24, 24, 491, 491, 827, 956, 356, 356, 356, 767, 491, 24, 491, 356, 356, 767, 491, 491, 491, 356, 767, 767, 767, 767, 767, 113, 767, 491, 491, 356, 491, 767, 491, 491, 491, 767, 767, 356, 491, 113, 491, 491, 491, 491, 491, 356, 767, 767, 491, 767, 767, 767, 356, 491, 491, 491, 24, 24, 24, 491, 356, 491, 767, 491, 356, 767, 767, 767, 258, 767, 767, 205, 767, 491, 356, 491, 491, 491, 767, 356, 767, 491, 767, 615, 356, 767, 767, 491, 767, 491, 356, 356, 491, 767, 615, 491, 356, 356, 956, 356, 767, 356, 24, 356, 356, 356, 356, 767, 491, 491, 491, 767, 767, 356, 24, 491, 767, 767, 491, 491, 491, 491, 491, 491, 767, 767, 491, 767, 767, 767, 24, 767, 767, 767, 356, 767, 491, 24, 113, 767, 767, 767, 767, 767, 767, 491, 767, 491, 767, 767, 491, 491, 491, 833, 574, 491, 753, 833, 51, 753, 753, 753, 768, 768, 491, 768, 491, 574, 833, 491, 768, 767, 491, 491, 491, 491, 767, 491, 491, 615, 491, 491, 767, 491, 767, 767, 767, 767, 767, 491, 767, 356, 767, 928, 491, 767, 767, 356, 491, 356, 767, 491, 491, 491, 767, 491, 491, 767, 767, 491, 491, 491, 356, 491, 767, 767, 491, 491, 491, 767, 491, 767, 615, 491, 491, 491, 491, 615, 753, 491, 491, 767, 392, 753, 491, 491, 491, 491, 491, 356, 491, 356, 491, 615, 651, 491, 491, 491, 767, 491, 491, 767, 491, 356, 491, 491, 753, 753, 491, 767, 928, 928, 356]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 2 / 20]
Batch Loss: 80.6242
Target: [7, 91, 700, 363, 779, 5, 211, 32, 7, 566, 118, 103, 32, 7, 58, 789, 779, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [356, 767, 767, 356, 767, 491, 767, 356, 356, 833, 833, 491, 491, 491, 491, 491, 491, 491, 767, 491, 767, 24, 767, 651, 491, 491, 767, 827, 767, 491, 491, 767, 767, 767, 767, 767, 491, 767, 491, 833, 356, 491, 767, 491, 24, 491, 491, 491, 491, 491, 491, 491, 491, 356, 356, 767, 491, 491, 118, 767, 767, 491, 491, 491, 356, 356, 491, 767, 767, 767, 767, 767, 767, 491, 491, 491, 491, 491, 767, 491, 491, 767, 767, 767, 491, 491, 491, 767, 767, 491, 767, 491, 491, 356, 767, 491, 767, 491, 491, 356, 956, 767, 767, 767, 767, 767, 491, 767, 767, 491, 491, 491, 956, 491, 491, 767, 491, 767, 491, 491, 491, 491, 491, 753, 491, 491, 491, 356, 491, 767, 767, 767, 767, 491, 491, 767, 491, 767, 491, 356, 491, 491, 767, 491, 24, 24, 767, 491, 491, 810, 810, 24, 767, 767, 491, 767, 491, 767, 491, 491, 491, 491, 356, 356, 491, 491, 767, 767, 767, 24, 491, 767, 767, 491, 491, 767, 491, 24, 24, 491, 491, 491, 767, 767, 767, 356, 767, 767, 767, 491, 767, 491, 24, 767, 767, 767, 767, 767, 491, 767, 767, 356, 491, 491, 491, 491, 767, 956, 491, 491, 491, 767, 767, 491, 491, 767, 767, 258, 93, 767, 491, 767, 356, 491, 491, 491, 491, 491, 767, 356, 491, 356, 118, 356, 767, 491, 491, 622, 767, 767, 491, 767, 767, 491, 767, 767, 767, 491, 767, 491, 491, 767, 767, 24, 24, 491, 651, 767, 24, 24, 767, 392, 767, 767, 767, 767, 767, 491, 767, 491, 24, 491, 767, 491, 491, 491, 356, 928, 491, 491, 491, 767, 491, 491, 491, 491, 767, 491, 491, 578, 651, 591, 753, 651, 491, 491, 651, 651, 767, 767, 491, 767, 491, 767, 753, 767, 491, 356, 491, 491, 491, 356, 491, 767, 491, 767, 767, 753, 767, 753, 491, 491, 833, 833, 767, 833, 491, 491, 491, 491, 767, 491, 767, 491, 491, 491, 767, 810, 491, 767, 491, 491, 491, 491, 491, 491, 356, 491, 24, 767, 491, 767, 491, 356, 767, 491, 356, 491, 767, 767, 767, 356, 491, 767, 491, 491, 93, 767, 767, 258, 356, 118, 491, 491, 356, 767, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 810, 767, 491, 491, 767, 24, 491, 258, 767, 767, 767, 767, 767, 767, 767, 491, 258, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 956, 491, 491, 491, 491, 491, 491, 767, 767, 491, 767, 767, 767, 753, 767, 753, 651, 767, 356, 767, 767, 651, 356, 356, 491, 491, 491, 491, 767, 491, 491, 491, 753, 753, 753, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 767, 767, 767, 491, 767, 491, 387, 767, 767, 491, 491, 491, 491, 491, 491, 491, 491, 356, 491]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 3 / 20]
Batch Loss: 77.4912
Target: [7, 25, 545, 51, 167, 17, 74, 132, 38, 9, 560, 404, 200, 34, 786, 410, 783, 281, 343, 37, 223, 650, 44, 235, 4, 70, 14, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [491, 491, 491, 767, 767, 767, 767, 833, 491, 356, 767, 491, 491, 767, 491, 767, 767, 767, 356, 356, 113, 491, 491, 491, 491, 491, 491, 491, 491, 93, 491, 767, 491, 356, 491, 491, 356, 491, 24, 491, 356, 491, 491, 767, 767, 491, 767, 356, 491, 491, 491, 491, 356, 356, 491, 767, 491, 118, 491, 491, 491, 491, 113, 113, 356, 956, 491, 356, 491, 491, 356, 767, 767, 767, 767, 491, 491, 491, 356, 491, 491, 767, 491, 491, 356, 356, 491, 205, 356, 767, 491, 491, 767, 491, 491, 24, 491, 491, 24, 24, 491, 767, 491, 491, 833, 833, 491, 491, 491, 491, 491, 491, 491, 574, 491, 491, 833, 491, 833, 491, 833, 767, 767, 833, 491, 574, 767, 491, 491, 491, 356, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 574, 833, 491, 491, 767, 491, 574, 574, 574, 767, 258, 767, 491, 767, 491, 767, 767, 491, 491, 356, 356, 491, 767, 767, 767, 767, 491, 491, 767, 767, 356, 491, 356, 767, 767, 767, 491, 767, 767, 491, 356, 767, 767, 491, 491, 491, 356, 491, 356, 767, 491, 491, 356, 491, 491, 767, 767, 767, 491, 491, 491, 491, 491, 356, 491, 356, 491, 491, 356, 356, 118, 491, 767, 356, 356, 356, 356, 118, 356, 491, 767, 491, 356, 491, 767, 491, 491, 491, 574, 24, 491, 767, 356, 491, 491, 491, 767, 491, 356, 356, 356, 491, 491, 767, 767, 491, 491, 491, 767, 767, 767, 767, 767, 767, 491, 356, 767, 491, 24, 491, 491, 491, 356, 356, 356, 356, 24, 356, 356, 356, 356, 356, 356, 356, 491, 356, 767, 767, 118, 767, 956, 767, 767, 767, 767, 356, 491, 491, 491, 767, 356, 491, 356, 93, 491, 491, 491, 118, 24, 24, 491, 491, 356, 767, 356, 491, 491, 491, 491, 491, 491, 491, 356, 491, 767, 615, 356, 767, 491, 356, 767, 356, 491, 767, 491, 491, 491, 491, 356, 356, 113, 491, 491, 491, 491, 491, 491, 833, 574, 574, 491, 767, 491, 833, 574, 491, 491, 833, 833, 491, 491, 767, 491, 491, 956, 767, 491, 491, 767, 356, 113, 767, 491, 491, 356, 356, 491, 491, 491, 491, 491, 767, 356, 356, 491, 491, 767, 491, 491, 491, 24, 491, 24, 356, 356, 113, 356, 356, 356, 491, 356, 356, 113, 491, 491, 356, 356, 356, 356, 356, 767, 767, 767, 767, 767, 491, 767, 767, 356, 767, 767, 767, 356, 767, 767, 767, 491, 491, 767, 491, 767, 833, 767, 767, 491, 767, 491, 767, 491, 767, 767, 491, 491, 767, 356, 491, 767, 767, 767, 491, 767, 491, 767, 767, 767, 356, 491, 491, 118, 491, 767, 491, 767, 767, 491, 767, 767, 956, 356, 491, 491, 767, 767, 767, 767, 491, 615, 356, 491, 491, 767, 491, 491, 356, 491, 491, 767, 356, 767, 491, 356, 491, 491, 491, 767, 491, 767, 767, 753, 491, 767, 491, 651, 491, 651, 753, 928, 928, 928]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 4 / 20]
Batch Loss: 81.5356
Target: [567, 194, 535, 763, 172, 5, 79, 157, 99, 774, 153, 665, 17, 117, 6, 168, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [356, 767, 767, 491, 767, 356, 491, 491, 491, 491, 767, 491, 767, 491, 939, 767, 767, 491, 24, 356, 491, 491, 491, 863, 491, 491, 24, 24, 767, 24, 767, 118, 767, 767, 491, 767, 24, 491, 491, 767, 491, 767, 767, 24, 767, 24, 767, 767, 24, 356, 767, 491, 356, 356, 113, 356, 491, 491, 491, 810, 24, 767, 767, 767, 491, 671, 574, 833, 671, 671, 833, 574, 491, 810, 767, 767, 491, 767, 956, 491, 833, 491, 491, 491, 491, 767, 767, 767, 767, 767, 767, 491, 491, 491, 356, 24, 491, 491, 118, 767, 356, 356, 491, 24, 491, 24, 491, 491, 24, 767, 491, 491, 767, 24, 24, 491, 491, 767, 491, 356, 491, 356, 491, 356, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 356, 767, 93, 767, 767, 491, 491, 767, 491, 767, 767, 491, 258, 767, 767, 491, 491, 491, 651, 491, 491, 491, 833, 574, 24, 491, 767, 767, 767, 24, 491, 491, 767, 767, 491, 767, 491, 491, 491, 491, 767, 491, 491, 356, 767, 574, 767, 392, 356, 491, 24, 491, 491, 753, 767, 491, 491, 24, 767, 491, 767, 767, 491, 767, 767, 767, 491, 491, 767, 574, 491, 767, 491, 767, 491, 651, 491, 491, 767, 491, 574, 767, 956, 491, 767, 491, 767, 767, 956, 767, 491, 767, 491, 956, 24, 767, 837, 767, 767, 491, 24, 24, 24, 767, 491, 767, 24, 491, 956, 491, 491, 767, 767, 767, 767, 767, 491, 491, 767, 491, 491, 767, 956, 767, 767, 491, 491, 767, 767, 767, 767, 93, 767, 901, 491, 767, 491, 767, 767, 491, 491, 491, 356, 491, 491, 356, 767, 767, 767, 767, 767, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 356, 258, 767, 833, 491, 491, 93, 491, 767, 491, 574, 491, 491, 491, 491, 767, 767, 767, 767, 491, 491, 356, 491, 767, 491, 767, 491, 767, 356, 491, 491, 113, 956, 491, 574, 767, 491, 833, 574, 767, 767, 767, 767, 767, 356, 767, 767, 767, 491, 767, 767, 491, 491, 491, 491, 491, 491, 767, 956, 767, 491, 939, 491, 24, 767, 767, 491, 491, 767, 491, 767, 767, 491, 767, 491, 767, 356, 491, 24, 767, 767, 767, 767, 753, 767, 356, 491, 767, 118, 671, 753, 767, 767, 671, 491, 574, 356, 767, 491, 491, 491, 767, 767, 767, 767, 767, 767, 767, 767, 491, 356, 491, 767, 491, 491, 491, 491, 491, 491, 491, 356, 767, 767, 767, 767, 356, 767, 767, 767, 767, 767, 491, 767, 615, 767, 767, 928, 767, 767, 491, 651, 767, 767, 491, 767, 491, 767, 767, 491, 767, 491, 767, 491, 491, 767, 928, 356, 767, 767, 767, 767, 491, 491, 767, 767, 491, 928, 767, 24, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 767, 491, 928, 356, 767, 767, 767, 491, 767, 767, 491, 928, 491, 356, 491, 491, 491, 767, 928, 767, 928, 491]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 5 / 20]
Batch Loss: 83.8959
Target: [144, 66, 787, 789, 779, 84, 153, 197, 659, 333, 49, 705, 23, 32, 17, 111, 119, 32, 15, 196, 30, 7, 630, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [491, 491, 491, 956, 491, 491, 956, 491, 491, 491, 491, 574, 753, 574, 24, 939, 491, 356, 491, 491, 491, 356, 491, 491, 491, 491, 491, 956, 491, 24, 767, 491, 956, 118, 93, 767, 491, 767, 356, 491, 491, 491, 491, 491, 491, 767, 574, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 767, 767, 491, 491, 491, 24, 356, 356, 491, 767, 491, 491, 356, 491, 24, 24, 767, 491, 491, 491, 767, 491, 767, 491, 767, 767, 356, 767, 491, 491, 491, 118, 491, 615, 622, 491, 24, 491, 622, 767, 356, 491, 491, 491, 356, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 24, 24, 24, 24, 491, 356, 491, 356, 491, 491, 491, 767, 491, 491, 491, 356, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 356, 24, 356, 767, 491, 491, 767, 491, 491, 491, 491, 491, 491, 651, 491, 491, 767, 767, 491, 767, 767, 491, 491, 356, 491, 491, 767, 491, 491, 767, 767, 491, 491, 491, 491, 356, 491, 172, 767, 356, 491, 491, 24, 491, 956, 574, 356, 491, 93, 767, 491, 356, 491, 491, 491, 491, 833, 491, 615, 491, 491, 24, 113, 491, 491, 118, 491, 767, 767, 491, 767, 767, 956, 356, 491, 491, 767, 767, 113, 491, 491, 93, 767, 767, 767, 491, 491, 767, 491, 491, 615, 24, 491, 767, 491, 767, 24, 767, 491, 491, 24, 491, 767, 24, 356, 491, 491, 356, 767, 491, 767, 491, 356, 356, 113, 491, 615, 491, 491, 491, 491, 491, 767, 767, 767, 24, 24, 356, 767, 491, 491, 491, 491, 767, 491, 767, 491, 767, 615, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 833, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 578, 491, 491, 24, 491, 491, 956, 491, 491, 491, 356, 491, 491, 491, 491, 767, 491, 491, 767, 767, 491, 767, 491, 491, 491, 491, 491, 491, 767, 491, 491, 767, 753, 491, 767, 491, 767, 491, 767, 767, 491, 491, 491, 491, 356, 491, 491, 767, 491, 491, 491, 491, 767, 491, 491, 491, 767, 767, 767, 767, 491, 24, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 767, 767, 491, 767, 767, 491, 491, 491, 491, 491, 491, 491, 767, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 356, 767, 491, 767, 491, 491, 491, 767, 491, 491, 767, 491, 491, 491, 491, 767, 767, 356, 491, 491, 767, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 767, 491, 356, 767, 767, 767, 767, 356, 767, 491, 491, 491, 491, 956, 767, 615, 767, 767, 767, 491, 767, 491, 491, 491, 491, 356, 491, 491, 767, 491, 491, 767, 491, 491, 491, 491, 753, 753, 491, 491, 491, 491, 491, 928, 356, 491, 491, 928]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 6 / 20]
Batch Loss: 82.7241
Target: [61, 44, 388, 68, 199, 735, 71, 41, 28, 656, 71, 317, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [928, 491, 810, 491, 956, 491, 491, 491, 671, 767, 491, 491, 491, 588, 578, 810, 810, 491, 491, 491, 491, 810, 491, 491, 491, 491, 491, 491, 258, 956, 753, 753, 753, 753, 753, 753, 753, 956, 753, 767, 574, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 118, 24, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 767, 767, 491, 767, 767, 491, 491, 356, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 767, 767, 767, 767, 767, 767, 767, 767, 767, 767, 356, 491, 767, 767, 491, 767, 767, 767, 491, 810, 113, 753, 491, 491, 491, 491, 491, 491, 491, 113, 491, 956, 356, 491, 491, 491, 767, 491, 491, 491, 118, 491, 491, 491, 491, 491, 767, 767, 24, 491, 491, 671, 51, 491, 574, 671, 768, 574, 767, 615, 615, 833, 356, 767, 767, 491, 491, 810, 491, 24, 491, 491, 491, 767, 491, 491, 491, 767, 767, 767, 767, 767, 491, 767, 491, 491, 767, 118, 118, 491, 767, 615, 491, 615, 767, 24, 767, 956, 491, 767, 356, 491, 491, 491, 93, 767, 615, 615, 615, 767, 356, 113, 118, 356, 24, 356, 24, 24, 24, 356, 491, 491, 491, 491, 491, 491, 491, 491, 810, 491, 767, 491, 491, 491, 491, 827, 491, 833, 810, 491, 491, 491, 753, 491, 491, 767, 810, 491, 491, 491, 491, 118, 491, 491, 491, 767, 491, 767, 491, 767, 491, 491, 491, 356, 356, 356, 767, 810, 118, 491, 24, 767, 118, 24, 356, 491, 24, 24, 24, 491, 491, 491, 491, 356, 491, 768, 768, 768, 768, 768, 768, 24, 651, 671, 651, 491, 767, 767, 574, 767, 491, 491, 356, 356, 491, 491, 491, 767, 356, 93, 491, 491, 767, 118, 356, 491, 491, 491, 827, 827, 491, 491, 491, 491, 491, 356, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 172, 356, 491, 491, 356, 356, 491, 491, 454, 24, 491, 24, 491, 356, 491, 24, 491, 491, 172, 491, 454, 356, 172, 827, 172, 172, 615, 454, 491, 172, 491, 615, 356, 356, 615, 356, 356, 491, 491, 491, 767, 753, 491, 767, 753, 491, 491, 767, 767, 767, 767, 767, 491, 767, 767, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 753, 491, 753, 491, 24, 24, 24, 24, 767, 491, 827, 491, 491, 615, 356, 356, 24, 491, 491, 491, 24, 928, 767, 24, 491, 491, 491, 767, 651, 491, 767, 753, 491, 491, 753, 651, 491, 651, 651, 651, 651, 651, 491, 491, 956, 491, 767, 753, 753, 767, 767, 767, 767, 753, 753, 753, 753, 753, 767, 767, 753, 491, 491, 491, 24, 491, 491, 491, 24, 767, 767, 24, 24, 491, 491, 491, 356, 491, 356, 24, 356, 491, 491, 491, 491, 491, 491, 753, 767, 491, 491, 753, 491, 651, 767, 356, 491, 491, 767, 491, 753, 491, 928]
----------------------------------------------------------------------------------------------------
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=80, out_features=40, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=40, out_features=1000, bias=True)
  )
  (test_gru): GRU(80, 40, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)
)
Model Parameters: 13402608
Features Shape: torch.Size([64, 80, 601])
Labels Shape: torch.Size([64, 39])
Features Length: tensor([601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,
        601, 601, 601, 601, 601, 601, 601, 601])
Shape: torch.Size([64])
Labels Length: tensor([20, 22, 16, 27, 24, 26, 22, 23, 39, 25, 19, 23, 25, 20, 24, 32, 19, 24,
        17, 35, 28, 23, 18, 21, 21, 26, 18, 12, 28, 28, 24, 21, 16, 24, 36, 19,
        27, 32, 22, 27, 22, 18, 21, 24, 20, 25, 18, 24, 19, 21, 21, 25, 24, 23,
        27, 27, 25, 24, 30, 24, 16, 18, 29, 26])
Shape: torch.Size([64])
[Epoch 1] - [Batch 7 / 20]
Batch Loss: 85.6536
Target: [289, 691, 9, 47, 189, 221, 343, 774, 3, 45, 373, 34, 210, 39, 9, 282, 73, 9, 282, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [491, 356, 767, 767, 491, 491, 491, 767, 491, 833, 491, 833, 491, 767, 491, 491, 767, 118, 356, 491, 356, 356, 24, 24, 356, 491, 491, 356, 491, 356, 491, 356, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 24, 491, 356, 356, 491, 491, 491, 491, 491, 833, 833, 833, 574, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 24, 356, 356, 491, 767, 491, 491, 356, 491, 491, 574, 491, 574, 491, 491, 767, 491, 491, 491, 491, 491, 574, 24, 827, 356, 491, 810, 491, 356, 491, 491, 491, 491, 113, 491, 767, 491, 491, 956, 356, 491, 491, 491, 491, 767, 578, 491, 827, 491, 491, 491, 491, 491, 491, 767, 24, 24, 491, 491, 767, 491, 491, 491, 767, 767, 491, 356, 767, 491, 491, 24, 491, 491, 491, 491, 956, 491, 574, 491, 491, 615, 833, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 615, 956, 356, 356, 356, 24, 24, 491, 767, 491, 491, 767, 491, 491, 491, 491, 767, 491, 24, 767, 491, 356, 356, 118, 810, 356, 767, 24, 24, 24, 356, 356, 392, 356, 356, 491, 491, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 491, 491, 491, 491, 491, 767, 491, 356, 491, 491, 491, 767, 491, 491, 491, 491, 753, 491, 767, 491, 767, 767, 491, 491, 767, 491, 956, 491, 491, 833, 767, 491, 767, 491, 491, 939, 491, 258, 767, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 491, 827, 24, 491, 491, 491, 956, 491, 491, 491, 491, 24, 491, 767, 356, 491, 24, 491, 491, 491, 491, 356, 491, 491, 491, 767, 767, 491, 24, 767, 491, 491, 767, 491, 113, 491, 767, 491, 491, 491, 491, 491, 491, 356, 356, 491, 491, 837, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 956, 767, 356, 491, 767, 767, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 24, 491, 491, 491, 491, 491, 356, 491, 491, 93, 491, 767, 767, 767, 767, 356, 356, 491, 491, 356, 767, 491, 356, 491, 356, 491, 491, 356, 767, 491, 491, 491, 491, 491, 491, 767, 767, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 767, 491, 356, 767, 491, 491, 491, 491, 956, 491, 491, 491, 356, 753, 491, 356, 833, 767, 574, 491, 578, 491, 491, 491, 491, 491, 491, 491, 491, 491, 574, 767, 491, 491, 491, 24, 356, 491, 491, 491, 356, 491, 491, 491, 491, 491, 767, 24, 491, 93, 767, 356, 491, 93, 491, 767, 491, 491, 491, 24, 491, 863, 491, 491, 491, 863, 356, 24, 93, 356, 356, 356, 356, 491, 356, 767, 767, 356, 491, 491, 767, 491, 491, 491, 491, 833, 356, 491, 24, 491, 833, 491, 833, 491, 356, 113, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 356, 356, 491, 491, 491, 491, 491, 491, 767, 767, 491, 491, 491, 491, 491, 767, 491, 24, 491, 767, 767, 491, 24, 24, 767, 491, 491, 491, 491, 491, 491, 356, 491, 491, 767, 491, 491, 767, 491, 356, 491, 491, 491, 387, 491, 767, 491, 356, 767, 767, 491, 491, 491, 491, 491, 491, 753, 491, 767, 956, 767, 767, 767, 767, 767, 767, 767, 833, 356, 767, 767, 767, 956, 491, 928, 356, 833, 258, 767, 767, 833, 767, 767, 833, 767, 767, 767, 833, 928, 767, 767, 767, 767, 767, 491, 491, 491, 767, 615, 767, 767, 491, 356, 767, 928, 928, 753, 491, 491, 491]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 8 / 20]
Batch Loss: 81.9639
Target: [5, 422, 45, 248, 482, 29, 72, 792, 39, 272, 145, 678, 123, 783, 608, 30, 109, 433, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [356, 491, 491, 491, 491, 356, 928, 356, 491, 356, 491, 356, 356, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 767, 491, 491, 118, 491, 356, 491, 491, 491, 491, 491, 491, 491, 118, 491, 491, 491, 491, 24, 491, 491, 491, 767, 767, 491, 491, 491, 491, 491, 491, 491, 491, 767, 356, 491, 491, 767, 491, 767, 767, 491, 491, 491, 767, 491, 491, 767, 491, 767, 24, 24, 491, 578, 491, 491, 491, 491, 491, 491, 767, 491, 491, 578, 578, 491, 491, 491, 24, 491, 276, 24, 24, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 767, 767, 767, 491, 491, 491, 491, 491, 454, 767, 491, 491, 767, 767, 491, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 491, 491, 491, 491, 276, 491, 491, 491, 709, 491, 491, 491, 491, 118, 767, 491, 113, 491, 491, 491, 24, 767, 491, 491, 767, 768, 768, 768, 768, 768, 753, 833, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 356, 356, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 491, 491, 491, 810, 491, 24, 491, 24, 491, 24, 491, 24, 491, 491, 491, 491, 491, 615, 491, 491, 491, 491, 491, 767, 113, 574, 491, 491, 651, 491, 574, 753, 651, 651, 753, 768, 651, 767, 753, 753, 753, 767, 491, 767, 491, 491, 491, 767, 491, 491, 491, 767, 491, 491, 491, 767, 491, 767, 491, 491, 491, 956, 767, 491, 491, 491, 491, 491, 767, 767, 767, 767, 767, 767, 767, 767, 491, 767, 24, 767, 491, 491, 491, 491, 491, 93, 491, 491, 491, 709, 767, 491, 767, 24, 491, 491, 491, 767, 491, 491, 491, 491, 491, 767, 767, 276, 491, 767, 356, 767, 491, 356, 491, 491, 491, 491, 491, 491, 491, 491, 24, 491, 491, 767, 491, 491, 491, 356, 356, 356, 491, 767, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 767, 356, 491, 767, 356, 491, 356, 356, 356, 491, 356, 491, 113, 491, 454, 491, 491, 491, 767, 491, 491, 491, 767, 491, 491, 491, 491, 24, 491, 768, 768, 768, 768, 491, 51, 491, 491, 491, 767, 767, 491, 491, 491, 491, 491, 491, 491, 651, 651, 491, 491, 491, 491, 24, 651, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 753, 928, 491, 356, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 24, 767, 24, 24, 24, 491, 767, 939, 491, 491, 491, 491, 956, 767, 491, 491, 491, 491, 356, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 833, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 767, 24, 491, 24, 491, 491, 491, 24, 491, 93, 491, 24, 24, 491, 491, 24, 24, 491, 24, 491, 767, 767, 767, 767, 767, 753, 356, 753, 491, 767, 767, 491, 767, 491, 491, 767, 767, 491, 491, 767, 651, 356, 491, 491, 356, 491, 491, 767, 491, 491, 491, 491, 753, 491, 356, 491, 491, 767, 753, 491, 651, 491, 491, 491, 491, 491, 928, 491, 928, 491, 491, 767, 491, 491, 491, 491, 491, 491, 767, 356, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 356, 767, 491, 491, 753, 491, 491, 491, 491, 767, 767, 491, 491, 491, 491, 753, 767, 491, 767, 767, 356, 491, 767, 491]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 9 / 20]
Batch Loss: 85.4448
Target: [46, 501, 775, 786, 176, 7, 21, 620, 96, 659, 333, 49, 322, 23, 32, 7, 510, 39, 30, 118, 298, 37, 690, 402, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [491, 491, 767, 491, 491, 491, 753, 491, 491, 491, 491, 491, 631, 491, 767, 767, 454, 491, 491, 491, 491, 491, 491, 356, 491, 356, 491, 491, 491, 491, 491, 24, 491, 491, 491, 491, 491, 491, 767, 491, 491, 24, 24, 491, 491, 24, 24, 491, 24, 939, 491, 767, 810, 767, 491, 491, 491, 491, 491, 767, 24, 671, 491, 24, 491, 491, 491, 24, 24, 24, 113, 491, 491, 491, 491, 491, 491, 491, 651, 491, 767, 767, 810, 491, 491, 113, 767, 491, 491, 491, 24, 491, 709, 356, 491, 767, 939, 767, 491, 491, 24, 574, 491, 24, 24, 491, 491, 491, 24, 491, 767, 491, 767, 767, 767, 622, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 118, 491, 767, 491, 767, 491, 491, 491, 491, 767, 767, 24, 491, 491, 491, 491, 491, 24, 491, 491, 491, 491, 491, 491, 24, 24, 24, 491, 491, 491, 491, 956, 356, 491, 491, 827, 491, 24, 454, 24, 491, 491, 24, 24, 491, 767, 767, 767, 24, 491, 491, 24, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 454, 24, 491, 491, 491, 767, 827, 24, 24, 767, 24, 767, 24, 491, 491, 454, 454, 491, 491, 491, 491, 491, 491, 767, 767, 118, 939, 767, 356, 491, 767, 118, 491, 491, 491, 491, 491, 491, 118, 454, 767, 491, 574, 491, 491, 491, 491, 491, 491, 491, 491, 827, 454, 810, 24, 24, 491, 491, 491, 491, 491, 491, 24, 491, 491, 491, 491, 491, 491, 491, 491, 491, 24, 491, 827, 767, 118, 356, 491, 491, 767, 767, 491, 767, 491, 491, 491, 24, 491, 491, 356, 767, 491, 24, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 356, 24, 491, 491, 335, 491, 491, 574, 258, 491, 767, 491, 491, 491, 767, 356, 491, 24, 615, 491, 491, 491, 491, 491, 767, 767, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 767, 356, 491, 491, 491, 491, 491, 491, 491, 491, 491, 454, 491, 491, 491, 491, 491, 491, 356, 491, 356, 956, 491, 118, 356, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 767, 768, 491, 491, 491, 491, 491, 491, 491, 356, 24, 767, 574, 767, 335, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 24, 491, 767, 767, 491, 491, 491, 491, 956, 491, 491, 491, 767, 24, 491, 454, 767, 491, 863, 356, 767, 671, 753, 491, 671, 768, 491, 768, 671, 113, 574, 768, 491, 491, 491, 491, 767, 491, 181, 651, 833, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 956, 491, 491, 767, 491, 491, 491, 956, 491, 491, 356, 767, 767, 491, 491, 491, 491, 491, 767, 491, 491, 356, 491, 491, 767, 767, 491, 767, 491, 491, 491, 767, 24, 491, 491, 491, 118, 491, 491, 491, 491, 491, 491, 491, 24, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 753, 491, 491, 24, 491, 491, 767, 491, 767, 491, 491, 491, 491, 491, 491, 258, 356, 491, 491, 491, 753, 767, 491, 767, 491, 491, 491, 651, 767, 491, 491, 767, 491, 491, 491, 753, 491, 491, 491, 491, 491, 491, 356, 767, 491, 767, 753, 491, 491, 615, 491, 491, 491, 491, 491, 767, 491, 615, 767, 491, 491, 491, 491, 753, 651, 491, 491, 491, 631, 491, 767, 491, 651, 491, 753, 491, 491, 356, 491, 491, 491, 928, 767, 767]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 10 / 20]
Batch Loss: 80.7628
Target: [61, 44, 7, 595, 9, 319, 32, 7, 755, 58, 55, 790, 4, 780, 792, 709, 309, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [491, 356, 491, 491, 753, 767, 491, 491, 491, 356, 491, 615, 615, 491, 454, 491, 491, 491, 491, 491, 491, 491, 356, 113, 767, 767, 767, 491, 767, 928, 491, 491, 356, 767, 356, 767, 491, 767, 491, 491, 491, 767, 491, 767, 491, 491, 767, 767, 767, 24, 767, 491, 356, 491, 491, 356, 928, 356, 491, 356, 356, 767, 491, 491, 491, 491, 491, 491, 356, 491, 767, 356, 356, 767, 491, 491, 356, 491, 753, 928, 928, 767, 767, 767, 24, 93, 24, 356, 118, 491, 491, 491, 491, 491, 810, 113, 491, 491, 491, 356, 356, 118, 113, 491, 491, 118, 491, 356, 491, 491, 491, 356, 491, 491, 356, 491, 491, 956, 491, 767, 491, 767, 491, 956, 767, 356, 491, 753, 767, 491, 753, 491, 928, 928, 491, 767, 767, 491, 491, 767, 615, 767, 767, 24, 491, 491, 491, 574, 767, 491, 491, 491, 491, 356, 767, 491, 767, 356, 767, 767, 491, 387, 356, 491, 356, 356, 491, 491, 767, 491, 767, 491, 491, 491, 491, 491, 767, 767, 767, 767, 833, 491, 356, 356, 491, 491, 956, 956, 491, 491, 767, 491, 491, 491, 24, 767, 827, 356, 356, 767, 356, 356, 356, 356, 491, 491, 491, 93, 454, 24, 491, 356, 356, 491, 615, 491, 767, 491, 113, 356, 24, 24, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 956, 767, 827, 767, 753, 491, 767, 491, 753, 753, 491, 767, 491, 491, 491, 491, 24, 491, 454, 491, 956, 767, 491, 356, 767, 356, 767, 491, 491, 767, 491, 491, 491, 93, 491, 491, 491, 491, 454, 454, 956, 491, 767, 767, 767, 356, 767, 767, 387, 767, 767, 356, 356, 767, 491, 491, 767, 767, 356, 491, 491, 491, 491, 491, 767, 491, 491, 118, 767, 118, 491, 491, 491, 356, 491, 491, 491, 491, 491, 491, 454, 491, 491, 491, 118, 491, 118, 491, 491, 491, 491, 767, 491, 753, 928, 753, 767, 753, 491, 356, 753, 491, 491, 767, 767, 956, 491, 356, 491, 767, 491, 491, 491, 767, 491, 491, 767, 491, 767, 491, 491, 491, 387, 491, 491, 767, 767, 767, 491, 491, 491, 491, 491, 491, 491, 491, 767, 767, 491, 491, 491, 491, 491, 491, 491, 113, 491, 767, 491, 356, 491, 491, 387, 491, 491, 651, 827, 651, 767, 753, 491, 491, 356, 491, 491, 753, 491, 491, 491, 491, 491, 767, 491, 491, 651, 491, 651, 356, 491, 491, 491, 491, 491, 491, 491, 356, 753, 491, 491, 491, 491, 491, 491, 356, 356, 491, 491, 767, 753, 491, 753, 491, 491, 615, 491, 491, 491, 356, 491, 491, 767, 767, 767, 491, 491, 491, 491, 491, 491, 651, 753, 356, 767, 491, 651, 356, 491, 928, 491, 928, 491, 491, 491, 491, 753, 491, 651, 767, 491, 651, 491, 491, 767, 491, 491, 491, 491, 491, 356, 651, 491, 767, 491, 491, 491, 491, 753, 491, 491, 491, 491, 491, 753, 491, 491, 356, 491, 767, 491, 491, 651, 491, 491, 491, 753, 767, 767, 491, 491, 651, 767, 491, 651, 491, 356, 491, 491, 651, 491, 491, 491, 753, 491, 767, 491, 491, 356, 753, 767, 491, 767, 767, 491, 387, 491, 651, 491, 491, 767, 491, 491, 356, 753, 651, 491, 491, 491, 651, 491, 491, 491, 491, 491, 753, 491, 651, 491, 767, 491, 491, 491, 651, 491, 491, 491, 491, 491, 767, 491, 491, 491, 928, 491, 491, 753, 767, 767, 491, 491, 491, 767, 767, 491, 651, 491, 491, 491, 491, 491, 356, 651, 491, 767, 491, 491, 491, 928]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 11 / 20]
Batch Loss: 85.5459
Target: [25, 28, 13, 70, 23, 795, 774, 291, 8, 693, 627, 44, 498, 73, 7, 12, 14, 27, 737, 129, 27, 12, 14, 27, 91, 154, 79, 779, 0, 0, 0, 0, 0] 
Predicted: [491, 491, 615, 356, 491, 767, 491, 753, 753, 491, 491, 753, 454, 491, 491, 491, 491, 491, 491, 491, 491, 837, 356, 356, 356, 356, 491, 767, 356, 491, 491, 491, 491, 247, 491, 491, 454, 768, 810, 491, 491, 491, 491, 768, 491, 827, 767, 767, 491, 356, 356, 491, 24, 24, 491, 356, 767, 24, 491, 118, 491, 491, 491, 491, 833, 574, 767, 768, 491, 491, 491, 491, 491, 768, 827, 827, 491, 491, 491, 491, 24, 24, 491, 767, 491, 491, 24, 767, 491, 491, 491, 491, 356, 491, 491, 356, 491, 24, 24, 24, 24, 491, 491, 491, 767, 491, 491, 356, 491, 671, 827, 491, 491, 491, 491, 491, 939, 491, 491, 491, 491, 810, 24, 767, 356, 24, 491, 956, 939, 356, 767, 356, 491, 491, 491, 491, 928, 356, 491, 491, 491, 491, 491, 491, 574, 491, 491, 491, 491, 356, 356, 491, 356, 93, 491, 356, 956, 356, 491, 24, 356, 24, 356, 24, 24, 356, 356, 491, 356, 24, 491, 863, 24, 491, 837, 671, 491, 491, 767, 767, 767, 491, 827, 491, 956, 491, 767, 491, 491, 356, 258, 939, 767, 356, 767, 24, 24, 491, 356, 827, 356, 491, 113, 118, 491, 491, 767, 24, 24, 24, 491, 356, 24, 491, 356, 491, 491, 491, 491, 767, 24, 24, 356, 24, 113, 24, 491, 827, 767, 356, 491, 491, 491, 767, 24, 767, 491, 767, 356, 24, 615, 491, 491, 356, 356, 356, 491, 356, 356, 578, 491, 615, 356, 24, 491, 356, 491, 491, 491, 491, 491, 491, 491, 768, 454, 491, 356, 491, 491, 491, 491, 24, 491, 24, 491, 491, 491, 491, 93, 767, 491, 491, 767, 356, 356, 491, 491, 767, 491, 767, 24, 24, 827, 491, 356, 356, 491, 356, 356, 356, 491, 491, 491, 491, 491, 491, 356, 491, 491, 767, 767, 356, 491, 491, 767, 767, 491, 491, 491, 491, 622, 24, 356, 356, 172, 491, 356, 356, 24, 356, 24, 356, 356, 24, 491, 491, 24, 491, 356, 491, 356, 356, 491, 827, 491, 491, 491, 833, 491, 768, 939, 491, 491, 491, 491, 491, 491, 491, 356, 767, 356, 615, 356, 356, 356, 491, 767, 767, 767, 356, 767, 356, 24, 767, 491, 767, 93, 767, 767, 491, 356, 491, 491, 491, 491, 356, 491, 491, 491, 491, 491, 491, 247, 491, 491, 491, 491, 24, 491, 491, 491, 491, 491, 491, 491, 491, 24, 491, 491, 491, 356, 491, 24, 491, 491, 356, 356, 767, 491, 833, 491, 491, 833, 491, 768, 720, 335, 491, 0, 767, 491, 491, 767, 24, 491, 491, 491, 118, 491, 491, 767, 491, 491, 491, 753, 491, 491, 491, 491, 491, 491, 753, 491, 24, 651, 767, 491, 491, 491, 833, 491, 491, 491, 753, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 767, 491, 767, 491, 491, 491, 767, 491, 491, 753, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 24, 767, 491, 491, 491, 767, 491, 753, 491, 491, 491, 767, 491, 767, 767, 491, 491, 491, 491, 491, 491, 491, 767, 753, 491, 491, 491, 767, 753, 491, 491, 491, 491, 491, 491, 767, 491, 491, 631, 491, 491, 753, 118, 491, 491, 491, 491, 767, 753, 753, 491, 753, 753, 767, 356, 491, 753, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 767, 491, 491, 767, 491, 356, 491, 491, 491, 767, 767, 356, 491, 491, 387, 491, 753, 767, 928, 767, 491, 491, 928, 491, 491]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 12 / 20]
Batch Loss: 85.0935
Target: [7, 543, 39, 470, 132, 84, 380, 149, 580, 4, 450, 793, 774, 20, 779, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [491, 491, 356, 491, 356, 356, 767, 767, 928, 356, 356, 454, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 768, 671, 491, 863, 491, 767, 491, 24, 491, 24, 491, 491, 491, 491, 491, 491, 491, 24, 767, 113, 491, 491, 491, 863, 118, 454, 491, 810, 767, 767, 491, 491, 24, 24, 491, 24, 24, 356, 491, 491, 767, 491, 356, 491, 491, 356, 356, 491, 956, 491, 491, 24, 491, 93, 491, 491, 24, 491, 24, 356, 491, 491, 491, 491, 491, 24, 491, 24, 113, 810, 113, 491, 356, 491, 956, 356, 491, 356, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 767, 767, 767, 491, 491, 767, 356, 491, 491, 491, 491, 837, 491, 491, 491, 863, 615, 24, 767, 491, 24, 810, 118, 118, 118, 491, 491, 454, 810, 491, 491, 671, 491, 491, 24, 491, 491, 767, 767, 768, 491, 767, 767, 767, 767, 956, 491, 491, 491, 491, 491, 491, 491, 491, 24, 118, 491, 491, 491, 24, 24, 491, 24, 491, 491, 491, 24, 491, 454, 810, 491, 491, 491, 24, 24, 356, 767, 491, 491, 491, 24, 491, 356, 93, 491, 491, 491, 767, 491, 491, 172, 113, 767, 491, 767, 491, 356, 767, 767, 491, 113, 767, 767, 93, 491, 956, 491, 491, 491, 356, 356, 491, 615, 491, 356, 491, 491, 491, 491, 578, 491, 491, 356, 491, 276, 491, 767, 24, 24, 578, 491, 810, 491, 24, 24, 491, 113, 491, 356, 767, 24, 356, 491, 24, 24, 491, 491, 491, 93, 24, 24, 24, 24, 767, 491, 709, 491, 491, 767, 767, 113, 24, 118, 491, 491, 810, 24, 24, 24, 24, 491, 356, 767, 767, 491, 491, 767, 491, 767, 491, 491, 491, 491, 491, 491, 356, 491, 491, 753, 491, 24, 491, 491, 491, 491, 491, 491, 491, 356, 356, 491, 767, 491, 491, 491, 356, 491, 491, 491, 491, 454, 491, 491, 491, 491, 24, 491, 615, 956, 454, 118, 491, 491, 767, 491, 356, 939, 491, 113, 615, 24, 491, 491, 491, 491, 491, 753, 491, 767, 767, 491, 491, 491, 767, 356, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 863, 491, 491, 491, 767, 939, 767, 939, 810, 24, 491, 491, 491, 491, 491, 491, 491, 767, 24, 491, 491, 767, 491, 491, 491, 24, 24, 491, 491, 767, 491, 24, 491, 491, 491, 491, 827, 356, 491, 491, 356, 491, 767, 491, 491, 356, 491, 491, 491, 491, 767, 118, 956, 491, 24, 356, 491, 491, 491, 578, 491, 956, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 491, 0, 491, 768, 768, 491, 768, 753, 768, 768, 768, 335, 768, 671, 671, 768, 768, 491, 491, 651, 651, 491, 491, 767, 491, 491, 753, 753, 767, 491, 491, 356, 454, 491, 491, 767, 356, 491, 956, 753, 491, 491, 615, 491, 827, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 767, 454, 491, 767, 615, 24, 491, 356, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 753, 956, 753, 491, 615, 491, 491, 491, 491, 491, 491, 392, 491, 356, 767, 491, 356, 767, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 767, 767, 491, 491, 491, 491, 356, 615, 491, 491, 767, 491, 491, 767, 491, 491, 491, 767, 491, 491, 356, 491, 767, 767, 753, 356, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 356, 767, 767, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 753, 928, 356, 356, 928]
----------------------------------------------------------------------------------------------------
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=80, out_features=40, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=40, out_features=1000, bias=True)
  )
  (test_gru): GRU(80, 40, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)
)
Model Parameters: 13402608
Features Shape: torch.Size([64, 80, 651])
Labels Shape: torch.Size([64, 34])
Features Length: tensor([651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651,
        651, 651, 651, 651, 651, 651, 651, 651])
Shape: torch.Size([64])
Labels Length: tensor([24, 28, 26, 19, 25, 27, 29, 22, 18, 24, 31, 30, 24, 26, 31, 19, 31, 17,
        24, 30, 19, 26, 21, 23, 25, 23, 19, 26, 26, 25, 29, 22, 33, 23, 17, 25,
        21, 31, 16, 33, 24, 17, 30, 26, 28, 22, 25, 25, 26, 31, 21, 20, 29, 26,
        23, 25, 21, 34, 32, 27, 25, 22, 24, 16])
Shape: torch.Size([64])
[Epoch 1] - [Batch 13 / 20]
Batch Loss: 86.7803
Target: [25, 154, 222, 325, 369, 789, 319, 199, 572, 141, 9, 290, 774, 44, 5, 773, 395, 34, 125, 79, 14, 340, 784, 37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [491, 491, 491, 491, 491, 491, 753, 767, 491, 491, 118, 491, 491, 491, 491, 491, 24, 491, 454, 491, 491, 491, 491, 491, 491, 491, 24, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 491, 491, 491, 491, 356, 491, 356, 615, 491, 491, 491, 491, 491, 491, 491, 491, 24, 491, 491, 491, 491, 491, 454, 118, 454, 491, 491, 491, 491, 491, 118, 491, 491, 491, 491, 491, 615, 491, 491, 767, 356, 356, 356, 491, 491, 491, 356, 454, 24, 491, 491, 454, 491, 491, 24, 491, 491, 491, 615, 491, 356, 491, 491, 956, 491, 491, 491, 118, 767, 491, 335, 24, 491, 491, 491, 491, 753, 491, 491, 753, 491, 491, 491, 491, 491, 491, 24, 454, 454, 491, 491, 118, 491, 491, 24, 454, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 356, 24, 491, 491, 118, 24, 491, 356, 118, 491, 491, 768, 768, 335, 768, 939, 768, 768, 113, 753, 491, 491, 615, 491, 491, 356, 956, 491, 491, 491, 491, 118, 491, 491, 491, 491, 118, 491, 833, 574, 491, 24, 491, 491, 491, 491, 767, 491, 491, 118, 118, 491, 491, 491, 767, 491, 767, 491, 491, 24, 454, 491, 956, 491, 491, 491, 491, 118, 491, 491, 113, 118, 491, 491, 491, 491, 491, 491, 491, 491, 491, 768, 768, 768, 491, 720, 574, 768, 491, 574, 491, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 767, 833, 491, 491, 491, 491, 753, 491, 767, 491, 491, 491, 491, 454, 491, 491, 491, 491, 574, 118, 491, 615, 767, 491, 356, 491, 113, 491, 356, 491, 454, 767, 356, 491, 118, 491, 491, 491, 491, 491, 24, 863, 93, 491, 356, 491, 118, 827, 491, 247, 491, 454, 454, 24, 118, 491, 113, 491, 491, 356, 491, 491, 491, 491, 356, 491, 956, 491, 491, 24, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 956, 491, 491, 113, 827, 767, 454, 491, 356, 827, 767, 356, 939, 356, 491, 491, 118, 491, 491, 491, 118, 491, 118, 491, 974, 491, 118, 491, 491, 356, 356, 491, 491, 491, 491, 491, 491, 491, 118, 491, 491, 491, 454, 491, 491, 454, 491, 491, 454, 767, 356, 491, 454, 491, 491, 491, 491, 491, 491, 491, 258, 491, 491, 491, 491, 767, 767, 491, 118, 615, 767, 491, 491, 356, 491, 753, 491, 491, 767, 491, 491, 491, 767, 767, 491, 491, 491, 491, 753, 491, 767, 491, 491, 491, 491, 491, 491, 767, 615, 491, 491, 491, 356, 118, 491, 491, 491, 491, 491, 491, 491, 491, 491, 356, 491, 491, 491, 491, 491, 753, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 753, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 767, 491, 491, 491, 24, 491, 491, 356, 118, 491, 491, 491, 767, 767, 454, 491, 24, 491, 491, 491, 491, 939, 491, 491, 491, 491, 491, 767, 753, 491, 491, 491, 491, 491, 956, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 767, 491, 118, 767, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 651, 491, 491, 491, 491, 356, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 827, 491, 491, 767, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 767, 767, 651, 491, 491, 753, 753, 491, 767, 651, 491, 491, 753, 491, 491, 491, 753, 491, 491, 767, 491, 491, 356, 491]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 14 / 20]
Model Summary: SpeechRecognitionModel(
  (rescnn_layers): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): CNNLayerNorm(
        (layer_norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=1280, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=1000, bias=True)
  )
)
Model Parameters: 14337608
Features Shape: torch.Size([64, 1, 80, 501])
Labels Shape: torch.Size([64, 30])
Features Length: tensor([501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
        501, 501, 501, 501, 501, 501, 501, 501])
Shape: torch.Size([64])
Labels Length: tensor([18, 19, 21, 18, 18, 16, 22, 26, 20, 21, 28, 30, 25, 20, 20, 21, 14, 21,
        15, 26, 25, 17, 15, 20, 16, 12, 17, 15, 19, 14, 19, 20, 20, 14, 19, 15,
        15, 19, 16, 27, 26, 24, 20, 28, 13, 29, 21, 22, 19, 14, 29, 29, 17, 23,
        30, 26, 18, 25, 20, 23, 20, 26, 22, 16])
Shape: torch.Size([64])
[Epoch 1] - [Batch 1 / 20]
Batch Loss: 82.8529
Target: [61, 53, 7, 483, 487, 131, 130, 60, 483, 11, 68, 91, 282, 490, 90, 54, 372, 409, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 
Predicted: [655, 654, 655, 405, 481, 850, 169, 169, 169, 905, 850, 110, 905, 435, 905, 905, 382, 850, 905, 850, 905, 850, 850, 617, 850, 850, 850, 850, 850, 435, 580, 873, 287, 451, 435, 850, 850, 850, 850, 169, 850, 850, 850, 850, 110, 169, 169, 169, 435, 850, 89, 850, 850, 850, 435, 850, 850, 741, 89, 947, 850, 850, 169, 169, 850, 850, 850, 169, 850, 850, 512, 548, 617, 850, 850, 850, 850, 645, 645, 850, 850, 169, 850, 850, 117, 169, 38, 850, 89, 89, 850, 850, 850, 850, 850, 645, 850, 382, 873, 268, 850, 873, 382, 850, 850, 850, 850, 850, 169, 110, 654, 873, 393, 850, 850, 850, 850, 850, 850, 850, 850, 104, 758, 169, 850, 905, 803, 758, 984, 873, 967, 950, 655, 117, 89, 512, 481, 110, 38, 873, 850, 873, 911, 905, 850, 850, 110, 110, 850, 110, 967, 850, 850, 850, 38, 850, 850, 850, 850, 850, 850, 850, 850, 645, 850, 110, 947, 850, 244, 850, 850, 850, 850, 850, 850, 850, 850, 169, 169, 850, 169, 850, 850, 850, 645, 850, 850, 850, 850, 117, 850, 169, 110, 265, 265, 360, 89, 850, 850, 905, 873, 655, 905, 905, 905, 850, 850, 169, 905, 850, 850, 655, 117, 110, 873, 110, 89, 110, 678, 905, 382, 117, 626, 992, 190, 850, 190, 110, 190, 110, 110, 916, 104, 104, 104, 117, 110, 320, 104, 876, 110, 110, 110, 110, 466, 104, 110, 110, 104, 110, 104]
----------------------------------------------------------------------------------------------------
[Epoch 1] - [Batch 2 / 20]
